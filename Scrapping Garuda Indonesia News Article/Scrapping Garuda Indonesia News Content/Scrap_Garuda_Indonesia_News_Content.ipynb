{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2IaRH3Uv8nZh3YDwFbQCl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renaldoaluska/pbagasal2025-klp11-garuda/blob/main/Scrapping%20Garuda%20Indonesia%20News%20Article/Scrapping%20Garuda%20Indonesia%20News%20Content/Scrap_Garuda_Indonesia_News_Content.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas tqdm requests newspaper3k trafilatura readability-lxml justext lxml-html-clean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r22I_civv_jg",
        "outputId": "2de2c462-9703-4137-e8ba-239e73f1ffe7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting trafilatura\n",
            "  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting readability-lxml\n",
            "  Downloading readability_lxml-0.8.4.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting justext\n",
            "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting lxml-html-clean\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (4.13.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.2)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (5.4.0)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (3.9.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting courlan>=1.3.2 (from trafilatura)\n",
            "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting htmldate>=1.9.2 (from trafilatura)\n",
            "  Downloading htmldate-1.9.3-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from readability-lxml) (5.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.15.0)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n",
            "  Downloading tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dateparser>=1.1.2 (from htmldate>=1.9.2->trafilatura)\n",
            "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (2024.11.6)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.19.1)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading readability_lxml-0.8.4.1-py3-none-any.whl (19 kB)\n",
            "Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading htmldate-1.9.3-py3-none-any.whl (31 kB)\n",
            "Downloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading tld-0.13.1-py2.py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=d6d21d741f36f28e2ae19c38cb1b76aac8ada88e65868644bd93a1fb925e36cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/91/9f/00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=0cfeee5b5dc5cf816ffccd2f7f35739ba0615406b8f65e98ae06d3d8f899fcf0\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/9f/fb/364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=584d56cbf3c9f9e0abac2c3fed988bf892fc1b88e11ef900ee4b33e3079caae5\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/72/f7/fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=09bae891d03f7472340826fc6c333c82edd02328a7d9c10fa497c000920c9036\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, tld, lxml-html-clean, feedparser, cssselect, requests-file, feedfinder2, dateparser, courlan, tldextract, readability-lxml, justext, htmldate, trafilatura, newspaper3k\n",
            "Successfully installed courlan-1.3.2 cssselect-1.3.0 dateparser-1.2.2 feedfinder2-0.0.4 feedparser-6.0.12 htmldate-1.9.3 jieba3k-0.35.1 justext-3.0.2 lxml-html-clean-0.4.2 newspaper3k-0.2.8 readability-lxml-0.8.4.1 requests-file-2.1.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tld-0.13.1 tldextract-5.3.0 trafilatura-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== (1) Install dependensi (jalankan sekali) ====\n",
        "# Di Colab aktifkan baris pip berikut; di environment lokal bisa pip install lewat terminal.\n",
        "# !pip install pandas tqdm requests newspaper3k trafilatura readability-lxml justext lxml-html-clean\n",
        "\n",
        "# ==== (2) Import ====\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "# parser utama & fallback\n",
        "from newspaper import Article\n",
        "import trafilatura\n",
        "from readability import Document\n",
        "from lxml import html, etree\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# ==== (3) Konfigurasi ====\n",
        "INPUT_CSV = \"data_link_berita.csv\"  # ganti kalau beda\n",
        "OUTPUT_CSV = None  # biarkan None agar otomatis\n",
        "LINK_COL = None     # biarkan None agar ambil kolom pertama\n",
        "TITLE_COL_NAMES = [\"judul\", \"title\", \"headline\"]  # daftar kandidat nama kolom judul\n",
        "TIMEOUT = 15\n",
        "RETRIES = 3\n",
        "SLEEP_BASE = 1.2     # jeda dasar antar request (hindari 429)\n",
        "SLEEP_JITTER = (0.0, 0.8)  # jitter tambahan\n",
        "USER_AGENTS = [\n",
        "    # putar beberapa UA agar lebih aman\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16 Safari/605.1.15\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118 Safari/537.36\",\n",
        "]"
      ],
      "metadata": {
        "id": "0MQmi1yP7NbV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_ZZSzi6vwak",
        "outputId": "8768329f-755b-479b-f35e-1322599a4d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Mengambil konten artikel: 100%|██████████| 614/614 [42:42<00:00,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selesai! Tersimpan: data_link_berita_with_content.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==== (4) Util: pilih kolom link & judul ====\n",
        "def pick_link_col(df: pd.DataFrame, prefer=LINK_COL):\n",
        "    if prefer and prefer in df.columns:\n",
        "        return prefer\n",
        "    return df.columns[0]\n",
        "\n",
        "def find_title_col(df: pd.DataFrame):\n",
        "    cols_lower = {c.lower(): c for c in df.columns}\n",
        "    for name in TITLE_COL_NAMES:\n",
        "        if name in cols_lower:\n",
        "            return cols_lower[name]\n",
        "    return None  # tidak ada judul\n",
        "\n",
        "# ==== (5) FungsI ambil HTML mentah dengan retry ====\n",
        "def fetch_html(url: str) -> str | None:\n",
        "    last_err = None\n",
        "    for attempt in range(1, RETRIES + 1):\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": random.choice(USER_AGENTS),\n",
        "                \"Accept-Language\": \"id,en;q=0.8\",\n",
        "                \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "            }\n",
        "            resp = requests.get(url, headers=headers, timeout=TIMEOUT)\n",
        "            # tangani rate limit\n",
        "            if resp.status_code == 429:\n",
        "                # exponential backoff kecil\n",
        "                time.sleep(SLEEP_BASE * attempt + random.uniform(*SLEEP_JITTER))\n",
        "                continue\n",
        "            resp.raise_for_status()\n",
        "            return resp.text\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            time.sleep(SLEEP_BASE * attempt + random.uniform(*SLEEP_JITTER))\n",
        "    return None\n",
        "\n",
        "# ==== (6) Ekstraksi judul & konten via beberapa metode ====\n",
        "def parse_with_newspaper(url: str):\n",
        "    try:\n",
        "        art = Article(url, keep_article_html=False, fetch_images=False)\n",
        "        art.download()\n",
        "        art.parse()\n",
        "        title = (art.title or \"\").strip()\n",
        "        text = (art.text or \"\").strip()\n",
        "        return title, text\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def parse_with_trafilatura(url: str, html_text: str | None = None):\n",
        "    try:\n",
        "        downloaded = html_text or trafilatura.fetch_url(url)\n",
        "        if not downloaded:\n",
        "            return None, None\n",
        "        text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)\n",
        "        # judul dari metadata\n",
        "        title = trafilatura.metadata.extract_metadata(downloaded)\n",
        "        title_txt = (title.title if title and title.title else \"\").strip()\n",
        "        return title_txt, (text or \"\").strip()\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def parse_with_readability(html_text: str):\n",
        "    try:\n",
        "        doc = Document(html_text)\n",
        "        title = (doc.short_title() or \"\").strip()\n",
        "        content_html = doc.summary()\n",
        "        # buang tag → teks\n",
        "        tree = html.fromstring(content_html)\n",
        "        text = \"\\n\".join([t.strip() for t in tree.xpath(\"//text()\") if t and t.strip()])\n",
        "        return title, text.strip()\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "def clean_text(txt: str) -> str:\n",
        "    if not txt:\n",
        "        return \"\"\n",
        "    # rapikan spasi berlebih\n",
        "    txt = re.sub(r\"\\s+\\n\", \"\\n\", txt)\n",
        "    txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt)\n",
        "    txt = re.sub(r\"[ \\t]{2,}\", \" \", txt)\n",
        "    return txt.strip()\n",
        "\n",
        "def extract_article(url: str):\n",
        "    # 1) coba newspaper3k\n",
        "    title, text = parse_with_newspaper(url)\n",
        "    if text and len(text.split()) >= 60:  # minimal panjang wajar\n",
        "        return clean_text(title), clean_text(text)\n",
        "\n",
        "    # 2) fetch html sekali agar hemat request\n",
        "    html_text = fetch_html(url)\n",
        "    if html_text:\n",
        "        # 2a) trafilatura\n",
        "        t2_title, t2_text = parse_with_trafilatura(url, html_text)\n",
        "        if t2_text and len(t2_text.split()) >= 60:\n",
        "            # pilih judul terbaik\n",
        "            best_title = title or t2_title\n",
        "            return clean_text(best_title), clean_text(t2_text)\n",
        "\n",
        "        # 2b) readability\n",
        "        r_title, r_text = parse_with_readability(html_text)\n",
        "        if r_text and len(r_text.split()) >= 60:\n",
        "            best_title = title or t2_title or r_title\n",
        "            return clean_text(best_title), clean_text(r_text)\n",
        "\n",
        "    # kalau semua gagal, kembalikan apa adanya (bisa kosong)\n",
        "    return clean_text(title), clean_text(text or \"\")\n",
        "\n",
        "# ==== (7) Proses CSV ====\n",
        "df = pd.read_csv(INPUT_CSV)\n",
        "link_col = pick_link_col(df)\n",
        "title_col = find_title_col(df)\n",
        "\n",
        "# siapkan kolom hasil\n",
        "if title_col is None:\n",
        "    title_col = \"judul\"\n",
        "    if title_col not in df.columns:\n",
        "        df[title_col] = \"\"\n",
        "\n",
        "df[\"konten\"] = \"\"\n",
        "\n",
        "# loop ambil artikel\n",
        "urls = df[link_col].astype(str).tolist()\n",
        "\n",
        "for i in tqdm(range(len(urls)), desc=\"Mengambil konten artikel\"):\n",
        "    url = urls[i]\n",
        "    if not isinstance(url, str) or not url.startswith(\"http\"):\n",
        "        continue\n",
        "    title, content = extract_article(url)\n",
        "    # isi judul hanya jika kosong atau null\n",
        "    if not isinstance(df.at[i, title_col], str) or not df.at[i, title_col].strip():\n",
        "        df.at[i, title_col] = title\n",
        "    df.at[i, \"konten\"] = content\n",
        "    # jeda kecil antar request (hindari 429)\n",
        "    time.sleep(SLEEP_BASE + random.uniform(*SLEEP_JITTER))\n",
        "\n",
        "# ==== (8) Letakkan 'konten' tepat di sebelah 'judul' ====\n",
        "def move_column_next_to(df: pd.DataFrame, col_to_move: str, target_col: str, after=True):\n",
        "    cols = list(df.columns)\n",
        "    if col_to_move not in cols or target_col not in cols:\n",
        "        return df\n",
        "    cols.remove(col_to_move)\n",
        "    idx = cols.index(target_col) + (1 if after else 0)\n",
        "    cols.insert(idx, col_to_move)\n",
        "    return df[cols]\n",
        "\n",
        "df = move_column_next_to(df, \"konten\", title_col, after=True)\n",
        "\n",
        "# ==== (9) Simpan ====\n",
        "if OUTPUT_CSV is None:\n",
        "    base, ext = os.path.splitext(INPUT_CSV)\n",
        "    # Change output extension to xlsx\n",
        "    OUTPUT_CSV = f\"{base}_with_content.xlsx\"\n",
        "\n",
        "df.to_csv(OUTPUT_CSV, index=False)\n",
        "print(f\"Selesai! Tersimpan: {OUTPUT_CSV}\")"
      ]
    }
  ]
}